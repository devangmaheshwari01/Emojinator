{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tJhAsnAZ-4L7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "# convolutional layers\n",
        "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "# fully connected layers\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "# output layer\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# Compile the model (make sure to compile the model with the same configuration as during training)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load the model from the H5 file\n",
        "model = load_model('emoji.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LxF7pzz0_lnK"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "def ef(image):\n",
        "    img = load_img(image)\n",
        "    feature = np.array(img)\n",
        "    feature = feature.reshape(1,48,48,3)\n",
        "    return feature/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rCQUU3lu_oeQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "# from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "import cv2\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ky92GekW_tbP"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TFFHJNZr_vY1"
      },
      "outputs": [],
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mbPN0Imt_wHg"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    // Function to remove DOM elements\n",
        "    function removeDom() {\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      video.remove();\n",
        "      div.remove();\n",
        "      video = null;\n",
        "      div = null;\n",
        "      stream = null;\n",
        "      imgElement = null;\n",
        "      captureCanvas = null;\n",
        "      labelElement = null;\n",
        "    }\n",
        "\n",
        "    // Function to handle key press events\n",
        "    document.addEventListener('keydown', function(e) {\n",
        "        if (e.key === \"Escape\") { // Check if the pressed key is 'Esc'\n",
        "            shutdown = true; // Set shutdown flag to true\n",
        "        }\n",
        "    });\n",
        "\n",
        "    // Function to handle animation frames\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Function to create DOM elements\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      // Create model status element\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"Status:\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      // Create video element\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      // Create image element\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      // Create instruction element\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '' +\n",
        "          'When finished, click here or on the video to stop this demo';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Create capture canvas element\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "\n",
        "    // Function to stream frame\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mgx4kMhzDEGH"
      },
      "outputs": [],
      "source": [
        "labels = {0:'angry',1:'disgust',2:'fear',3:'happy',4:'neutral',5:'sad',6:'surprise'}\n",
        "# import emoji\n",
        "# labels = {0:'angry ðŸ˜¡',1:'disgust ðŸ¤¢',2:'fear ðŸ˜°',3:'happy ðŸ˜Š',4:'neutral ðŸ˜',5:'sad ðŸ˜”',6:'surprise ðŸ˜¯'}\n",
        "# labels = {0:\"angry+str(emoji.emojize(':angry:'))\",1:\"disgust+str(emoji.emojize(':disgust:'))\",2:\"fear+str(emoji.emojize(':fear:'))\",\n",
        "#           3:\"happy+str(emoji.emojize(':happy:'))\",4:\"neutral+str(emoji.emojize(':neutral:'))\",5:\"sad+str(emoji.emojize(':sad:'))\",\n",
        "#           6:\"surprise+str(emoji.emojize(':surprise:'))\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eMS62M6dFOYE"
      },
      "outputs": [],
      "source": [
        "map = {0:'angry.png',1:'disgust.png',2:'fear.png',3:'happy.png',4:'neutral.png',\n",
        "       5:'sad.png',6:'surprise.png'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wxCC2Puc_zen",
        "outputId": "c091330a-52ae-4b89-b280-293ba95e5048"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    var video;\n    var div = null;\n    var stream;\n    var captureCanvas;\n    var imgElement;\n    var labelElement;\n\n    var pendingResolve = null;\n    var shutdown = false;\n\n    // Function to remove DOM elements\n    function removeDom() {\n      stream.getVideoTracks()[0].stop();\n      video.remove();\n      div.remove();\n      video = null;\n      div = null;\n      stream = null;\n      imgElement = null;\n      captureCanvas = null;\n      labelElement = null;\n    }\n\n    // Function to handle key press events\n    document.addEventListener('keydown', function(e) {\n        if (e.key === \"Escape\") { // Check if the pressed key is 'Esc'\n            shutdown = true; // Set shutdown flag to true\n        }\n    });\n\n    // Function to handle animation frames\n    function onAnimationFrame() {\n      if (!shutdown) {\n        window.requestAnimationFrame(onAnimationFrame);\n      }\n      if (pendingResolve) {\n        var result = \"\";\n        if (!shutdown) {\n          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n        }\n        var lp = pendingResolve;\n        pendingResolve = null;\n        lp(result);\n      }\n    }\n\n    // Function to create DOM elements\n    async function createDom() {\n      if (div !== null) {\n        return stream;\n      }\n\n      div = document.createElement('div');\n      div.style.border = '2px solid black';\n      div.style.padding = '3px';\n      div.style.width = '100%';\n      div.style.maxWidth = '600px';\n      document.body.appendChild(div);\n\n      // Create model status element\n      const modelOut = document.createElement('div');\n      modelOut.innerHTML = \"Status:\";\n      labelElement = document.createElement('span');\n      labelElement.innerText = 'No data';\n      labelElement.style.fontWeight = 'bold';\n      modelOut.appendChild(labelElement);\n      div.appendChild(modelOut);\n\n      // Create video element\n      video = document.createElement('video');\n      video.style.display = 'block';\n      video.width = div.clientWidth - 6;\n      video.setAttribute('playsinline', '');\n      video.onclick = () => { shutdown = true; };\n      stream = await navigator.mediaDevices.getUserMedia(\n          {video: { facingMode: \"environment\"}});\n      div.appendChild(video);\n\n      // Create image element\n      imgElement = document.createElement('img');\n      imgElement.style.position = 'absolute';\n      imgElement.style.zIndex = 1;\n      imgElement.onclick = () => { shutdown = true; };\n      div.appendChild(imgElement);\n\n      // Create instruction element\n      const instruction = document.createElement('div');\n      instruction.innerHTML =\n          '' +\n          'When finished, click here or on the video to stop this demo';\n      div.appendChild(instruction);\n      instruction.onclick = () => { shutdown = true; };\n\n      video.srcObject = stream;\n      await video.play();\n\n      // Create capture canvas element\n      captureCanvas = document.createElement('canvas');\n      captureCanvas.width = 640;\n      captureCanvas.height = 480;\n      window.requestAnimationFrame(onAnimationFrame);\n\n      return stream;\n    }\n\n    // Function to stream frame\n    async function stream_frame(label, imgData) {\n      if (shutdown) {\n        removeDom();\n        shutdown = false;\n        return '';\n      }\n\n      var preCreate = Date.now();\n      stream = await createDom();\n\n      var preShow = Date.now();\n      if (label != \"\") {\n        labelElement.innerHTML = label;\n      }\n\n      if (imgData != \"\") {\n        var videoRect = video.getClientRects()[0];\n        imgElement.style.top = videoRect.top + \"px\";\n        imgElement.style.left = videoRect.left + \"px\";\n        imgElement.style.width = videoRect.width + \"px\";\n        imgElement.style.height = videoRect.height + \"px\";\n        imgElement.src = imgData;\n      }\n\n      var preCapture = Date.now();\n      var result = await new Promise(function(resolve, reject) {\n        pendingResolve = resolve;\n      });\n      shutdown = false;\n\n      return {'create': preShow - preCreate,\n              'show': preCapture - preShow,\n              'capture': Date.now() - preCapture,\n              'img': result};\n    }\n\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "name 'eval_js' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     js_reply \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_html\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m js_reply:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[10], line 152\u001b[0m, in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvideo_frame\u001b[39m(label, bbox):\n\u001b[0;32m--> 152\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[43meval_js\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream_frame(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(label, bbox))\n\u001b[1;32m    153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_js' is not defined"
          ]
        }
      ],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# # Initialize the webcam\n",
        "# webcam = cv2.VideoCapture(0)\n",
        "\n",
        "# # Set the frame width and height\n",
        "# webcam.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "# webcam.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "# Load the face cascade classifier\n",
        "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "face_cascade = cv2.CascadeClassifier(haar_file)\n",
        "\n",
        "# Define a function to extract features\n",
        "def extract_features(image):\n",
        "    if len(image.shape) == 2:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    resized_image = cv2.resize(image, (48, 48))\n",
        "    feature = resized_image / 255.0\n",
        "    feature = np.expand_dims(feature, axis=0)\n",
        "    return feature\n",
        "\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # detect faces in the grayscale frame\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    # iterate over detected faces\n",
        "    for (x, y, w, h) in faces:\n",
        "        # extract the face region\n",
        "        face_image = gray[y:y+h, x:x+w]\n",
        "\n",
        "        # resize the face image to the required input size\n",
        "        resized_image = cv2.resize(face_image, (48, 48))\n",
        "\n",
        "        # extract features and normalize\n",
        "        input_data = extract_features(resized_image)\n",
        "\n",
        "        # make a prediction using the model\n",
        "        prediction = model.predict(input_data)\n",
        "        predicted_label = labels[prediction.argmax()]\n",
        "        # Load the emoji image\n",
        "        img_name=str(map[prediction.argmax()])\n",
        "        emoji_img = cv2.imread(img_name)\n",
        "        if emoji_img is not None:\n",
        "            emoji_resized = cv2.resize(emoji_img, (w, h))\n",
        "\n",
        "            # # Draw the emoji onto the frame\n",
        "            # for i in range(emoji_resized.shape[0]):\n",
        "            #     for j in range(emoji_resized.shape[1]):\n",
        "            #         # if emoji_resized[i, j, 2] != 0:  # Check alpha channel\n",
        "            #         bbox_array[y + i, x + j] = emoji_resized[i, j]\n",
        "            cv2_imshow(emoji_img)\n",
        "        else:\n",
        "            print(\"Error: Unable to load the image.\")\n",
        "\n",
        "        # draw a rectangle around the face\n",
        "        bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "        # display the predicted emotion label on the frame\n",
        "        cv2.putText(bbox_array, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Check for key press (ESC to exit)\n",
        "    key = cv2.waitKey(1)\n",
        "    if key == 27:  # ESC key\n",
        "        break\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "\n",
        "    # Display the frame\n",
        "    # cv2.imshow(\"Emotion Detection\", bbox_bytes)\n",
        "\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "# close OpenCV windows\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
